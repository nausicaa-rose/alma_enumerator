# -*- coding: utf-8 -*-

from settings import api_key
import requests
import re
from bs4 import BeautifulSoup

  ############################################           
 # Functions for getting record information #
############################################


class Item:
    def __init__(self, id_, description):
        self.id = id_
        self.description = description
        self.parse_me = self.split_description()
        self.enumeration_a = ''
        self.enumeration_b = ''
        self.chronology_i = ''
        self.chronology_j = ''
        self.chronology_k = ''
        self.error = False
        
    def __repr__(self):
        return '<Item({}, {}>'.format(self.id, self.description)
        
    def __str__(self):
        return '\n'.join(['{{id: {}' .format(self.id),
                          'description: {}'.format(self.description),
                          'parse_me: {}'.format(self.parse_me),
                          'enumeration_a: {}'.format(self.enumeration_a),
                          'enumeration_b: {}'.format(self.enumeration_b),
                          'chronology_i: {}'.format(self.chronology_i),
                          'chronology_j: {}'.format(self.chronology_j),
                          'chronology_k: {}'.format(self.chronology_k),
                          'error: {}}}'.format(self.error)])
                    
    def split_description(self):
        splitter_p = re.compile(r'( |\.)')
        
        return splitter_p.split(self.description)

        

def get_holdings(base_url, mms_id, api_key):
    """
    Get the holdings id(s) for the bib record via the Alma API.
    """
    holdings_list = []
    query = 'bibs/{}/holdings?apikey={}'
    r = requests.get(''.join([base_url, query.format(mms_id, api_key)]))
    soup = BeautifulSoup(r.text, 'lxml')
    holdings = soup.find_all('holding_id')
    for id in holdings:
        holdings_list.append(id.text)
        
    return holdings_list


def get_item_info(base_url, mms_id, holdings_id):
    """
    Get the enumeration, chronology, and item id for each item for each holdings
    record.
    """
    limit = 100
    offset = 0
    query = 'bibs/{}/holdings/{}/items?limit={}&offset={}&apikey={}'
    items_out = []

    r = requests.get(''.join([base_url, query.format(mms_id, holdings_id, limit, offset, api_key)]))
    soup = BeautifulSoup(r.text, 'xml')
    
    current_response = soup.find_all('item_data')
    items = current_response
    
    # Iterate through the entire list of items
    while True:
        if len(current_response) == limit:
            offset += limit
            r = requests.get(''.join([base_url, query.format(mms_id, holdings_id, limit, offset, api_key)]))
            soup = BeautifulSoup(r.text, 'xml')
            current_response = soup.find_all('item_data')
            items += current_response
        else:
            break
            

    for item in items:
        items_out.append(Item(item.find('handle_alma_generated_descriptions(item)pid').text, item.find('description').text))
    
    # Call get_info_from_description() function to parse the description
    # and return a dictionary with
    for i in items_out:
        items_out[items_out.index(i)] = get_info_from_description(i)         
                          
    return items_out 
    

def handle_alma_generated_descriptions(item):
    # Used to filter out Alma-generated descriptions. Since they already
    # have enumeration/chronology and have a funky format, we can ignore 
    # them for now.
    alma_generated_p = re.compile(r'.*:.*\(.*:.*\)')
    
    # Mark  descriptions generated by Alma's item prediction functionality
    # as errors.
    if alma_generated_p.match(item.description):
        item.error = True
    
def remove_unwanted_text(item):
    return item


def regularize_ranges(item):
    return item


def handle_year_month_pattern(item):
    return item


def handle_bad_spacing(item):
    return item


def convert_months_seasons(item):
    return item    

def get_info_from_description(item):
    """
    This is where most of the magic happens. If something goes wrong,
    it probably went wrong here.
    
    This function parses item descriptions and is called by get_item_info(). 
    For those descriptions it can parse, it returns a dictionary with each field
    converted to a format compatible with Alma's enumeration and chronology fields.
    """

    # This dictonary of patterns is used convert month and season words to numerals.
    # It's also used to identify descriptions with words that are not
    # month or season indicators.
    date_patterns = {
                     re.compile(r'(Jan\.?|January)', re.IGNORECASE):     '01',
                     re.compile(r'(Feb\.?|February)', re.IGNORECASE):    '02',
                     re.compile(r'(Mar\.?|March)', re.IGNORECASE):       '03',
                     re.compile(r'(Apr\.?|April)', re.IGNORECASE):       '04',
                     re.compile(r'May', re.IGNORECASE):                  '05',
                     re.compile(r'(Jun\.?|June)', re.IGNORECASE):        '06',
                     re.compile(r'(Jul\.?|July)', re.IGNORECASE):        '07',
                     re.compile(r'(Aug\.?|August)', re.IGNORECASE):      '08',
                     re.compile(r'(Sep\.?|Sept\.?|September)', re.IGNORECASE):   '09',
                     re.compile(r'(Oct\.?|October)', re.IGNORECASE):     '10',
                     re.compile(r'(Nov\.?|November)', re.IGNORECASE):    '11',
                     re.compile(r'(Dec\.?|December)', re.IGNORECASE):    '12',
                     re.compile(r'(Spr\.?|Spring)', re.IGNORECASE):      '21',
                     re.compile(r'(Sum\.?|Summer)', re.IGNORECASE):      '22',
                     re.compile(r'(Fal\.?|Fall|Autumn)', re.IGNORECASE): '23',
                     re.compile(r'(Win\.?|Winter)', re.IGNORECASE):      '24',
                     }                
    
    # This pattern is used to see if a field in info has numerals
    has_digits_p = re.compile(r'.*\d+')
    
    # This pattern is used to remove periods, parentheses, and commas that can
    # mess things up if they're left in.
    bad_chars_p = re.compile(r'[\.\(\),\\:]')

    # This pattern matches hyphens and slashes and is used to recognize and edit
    # date and issue ranges like 1995-1999, 12/13, or 5&6.
    joined_p = re.compile(r'[-/&]')
    
    # This pattern matches a field that is a single hyphen, slash, or ampersand.
    lone_joiner_p = re.compile(r'^[-/&]$')
    
    # This pattern is used to catch strings like 2011-Win or 2011/Win.
    year_mo_p = re.compile(r'(\d+)(-|/|&)([a-zA-Z]+)')
    
    # This pattern is used to distinguish between years and volume/issue numbers,
    # but it may trip over long continuously numbered issues. The pattern also
    # assumes it won't be handling records from before the 19th century.
    is_year_p = re.compile(r'(18|19|20)\d\d')
    
    # Used to filter out Alma-generated descriptions. Since they already
    # have enumeration/chronology and have a funky format, we can ignore 
    # them for now.
    alma_generated_p = re.compile(r'.*:.*\(.*:.*\)')
    
    
    bad_ends_begins = ('-', '%', '/')
    
    # Mark  descriptions generated by Alma's item prediction functionality
    # as errors.
    if alma_generated_p.match(item.description):
        item.error = True
    # Otherwise, keep cleaning the data.
    else:
        # TODO
        # Split on periods to avoid bad spacing like Dec.1997
        
        
        # This is used to collect all fields that appear to be words, but not
        # month or season words.
        to_remove = []
        # Check each field in info
        for i in item.parse_me:
            x = item.parse_me.index(i)
            # Scrub '.(),:' from text for better matching
            item.parse_me[x] = bad_chars_p.sub('', i)
            i = bad_chars_p.sub('', i)
            
            # Scrub '.(),:' from text for better matching
            item.parse_me[x] = bad_chars_p.sub('', i)
            i = bad_chars_p.sub('', i)
            # Remove leading hyphen
            if i.startswith(bad_ends_begins):
                item.parse_me[x] = i[1:]
                i = i[1:]
            # Remove trailing hypen
            if i.endswith(bad_ends_begins):
                item.parse_me[x] = i[:-1]
                i = i[:-1]       
            
            # If the item has numbers, sanitize it so that it consists of only
            # numerals and slashes
            if has_digits_p.match(i) != None:
                item.parse_me[item.parse_me.index(i)] = snarf_numerals(i)
                   
            # Find fields that include only alphabetic characters
            elif has_digits_p.match(i) == None and lone_joiner_p.match(i) == None:
                is_ok = False
                for key in date_patterns:
                    # If the field is in date_patterns, it's an indicator of month
                    # or season. Everything's good and we move on to the next field.
                    if key.match(i):
                        is_ok = True
                        break
                
                # If the field didn't match any of the date_patterns, it is probably
                # a descriptive word like 'Abstracts', 'INDEX', etc, or a 
                # volume/number indicator like 'v.', 'no.', etc. If that's the 
                # case, add it to the removal list.
                if is_ok == False:
                    to_remove.append(i)
        
        # Remove fields from info that we don't want.
        for i in to_remove:
            item.parse_me.remove(i)
        
        
        # Sometimes, we might encounter a description like 'v 46 July 2005-June 2006',
        # where there is no space surrounding the hyphen (or slash), which produces
        # a field that looks like this '2005-June', which will not process correctly.
        # To deal with this we split the field to look like ('2005', '-', 'June'),
        # drop the original field, and put the three new fields in its place.
        for i in item.parse_me:
            if year_mo_p.match(i) != None:
                index = item.parse_me.index(i)
                head = item.parse_me[0:index]
                tail = item.parse_me[(index + 1):]
                body = list(year_mo_p.match(i).groups())
                item.parse_me = head + body + tail
                
        
        #TODO
        # Make sure all ranges are by like ranges, so a pattern like ['June', '2005', '-', 'May', '2006' ]
        # becomes ['June/May', '2005/2006']
                
                
        # TODO
        # Now that we have all the fields set appropriately, join together
        # ranges into a uniform field/field format like 03/04 or 1995/1996.
            
    if item.error:
        pass
    else:                        
        mo_season = []
        years = []
        delete_me = []
        has_chron_k = False
        for i in item.parse_me:
            print(i)
            if lone_joiner_p.match(i):
                print('rexp')
                delete_me.append(i)
            elif not has_digits_p.match(i):
                print('mo')
                mo_season.append(i)
                print(mo_season)
                delete_me.append(i)
                look_ahead = item.parse_me[item.parse_me.index(i) + 1]
                if has_digits_p.match(look_ahead) and not is_year_p.match(look_ahead):
                    has_chron_k = True
            else:
                item.parse_me[item.parse_me.index(i)] = snarf_numerals(i)
                i = snarf_numerals(i)
                print('else')
                if is_year_p.match(i):
                    print('year')
                    years.append(i)
                    print(years)
                    delete_me.append(i)
                   
        for i in delete_me:
            item.parse_me.remove(i)
                   
        years = remove_duplicates(years)
        mo_season = remove_duplicates(mo_season)
           
        if len(years) == 1:
            item.chronology_i = years[0]
        elif len(years) > 1:
            item.chronology_i = '/'.join(years)
           
        if len(mo_season) == 1:
            item.chronology_j = mo_season[0]
        elif len(mo_season) > 1:
            item.chronology_j = '/'.join(mo_season)
           
        i_len = len(item.parse_me)  
           
                   
        if i_len > 0:
            item.enumeration_a = item.parse_me[0]
    
            if i_len > 1:
                if has_chron_k and i_len == 2:
                    item.chronology_k = item.parse_me[1]
                else:
    
                    item.enumeration_b = item.parse_me[1]
    
                if i_len == 3:
                    item.chronology_k = item.parse_me[2]
    
                if i_len > 3:
                    item.chronology_k = '/'.join(item.parse_me[2:])

        return item


def remove_duplicates(l):
    check_list = []
    out_list = [x for x in l if not (x in check_list or check_list.append(x))]

    return out_list
            
def handle_record_error(item, item_info):
    print('{} appears to be irregular. Please correct by hand.'.format(item))
    item_info['error'] = item
    return item_info
    
    
def snarf_numerals(string):
    """
    This function is called by get_info_from_description(). It takes a string
    of arbitrary characters and returns only those characters that are numerals 
    or a slash. Hyphens in input are converted to slashes. All other characters
    are discarded. Input like 'v.40-41' would be returned as '40/41'.
    """
    # This pattern matches hyphens and slashes and is used to recognize and edit
    # date and issue ranges like 1995-1999 or 12/13.
    rp = re.compile(r'[-/&]')
    
    numerals = rp.sub('/', ''.join([x 
                                    for x 
                                    in string
                                    if x.isdigit() or rp.match(x) != None]))
        
    return numerals
        
        
def write_header_to_csv(output_file, item_info, delimeter=','):
    """
    Write out the field headers: id, enumeration_a, etc, to the output file.
    """
    with open(output_file, 'a', encoding='utf-8') as fh:
        fh.write('{}\n'.format(delimeter.join(item_info[0].keys())))
        
    
def output_to_csv(output_file, error_file, item_info, delimeter=','):
    """
    Write each item's info as a row in our output file. Write records with
    errors to our error file.
    """
    item_errors = []
    # Write out the data we were able to extract
    with open(output_file, 'a', encoding='utf-8') as fh:
        for item in item_info:
            # Collect the descriptions we couldn't handle for writing to a 
            # seperate file.
            if 'error' in item:
                item_errors.append(item)
            else:                
                fh.write('{}\n'.format(delimeter.join(item.values())))
                
    # Write out the item id and description for those descriptions that we couldn't handle
    with open(error_file, 'a', encoding='utf-8') as fh:
        for item in item_errors:
            fh.write('{}\n'.format(delimeter.join(item.values())))
            
            
def fetch(mms_id, output_file, error_file, api_key, base_url):    
    # Get holdings id(s)
    holdings = get_holdings(base_url, mms_id, api_key)
    
    # Make sure the output file is empty.
    with open(output_file, 'w', encoding='utf-8') as fh:
        fh.truncate()
    
    # For each holdings ID, write the ID to the output file, get the information
    # for all of the holdings' items, write the field headers to the output file,
    # then write the item information to the output file and write errors to the 
    # error file.
    for h in holdings:
        with open(output_file, 'a', encoding='utf-8') as fh:
            fh.write('{}\n'.format(h))
    
        item_info = get_item_info(base_url, mms_id, holdings)
        write_header_to_csv(output_file, item_info)
        output_to_csv(output_file, error_file, item_info)
            
            
  ##################################           
 # Functions for updating records #
##################################

def get_info_from_csv(input_file):
    holdings_p = re.compile(r'^\d{16,16}$')
    header_p = re.compile(r'^[a-z_,]+$')
    items_p = re.compile(r'^[0-9,/]+$')
    item_info = {}
    with open(input_file, 'r', encoding='utf-8') as fh:
        lines = fh.readlines()

    """
    The loop below creates a data structure that looks like this:
        {holdings_id_number: 
            [ {id: number,
               enumeration_a: number,
               enumeration_b: number,
               chronology_i: number,
               ...
              },
              {id: number,
              ...
              }
            ],
         another_holdings_id_number:
             [ ... ],
        ...
        }
    Usually, this will create a dictionary with one holdings id
    and a list of item dictionaries, but some bibliographic records
    may have more than one holdings.
    """
    for line in lines:
        line = line.strip()
        if holdings_p.match(line):
            holdings_id = line
            item_info[holdings_id] = []
        elif header_p.match(line):
            keys = line.split(',')
        elif items_p.match(line):
            info = line.split(',')
            item_info[holdings_id].append(dict(zip(keys, info)))
            
    return item_info
    

def get_item_xml(base_url, mms_id, holdings_id, item_id, api_key):
    query = 'bibs/{}/holdings/{}/items/{}?apikey={}'
    r = requests.get(''.join([base_url, query.format(mms_id, holdings_id, item_id, api_key)]))
    item_xml = r.text
    
    return item_xml
    

def update_item_xml(item_xml, item_info):
    soup = BeautifulSoup(item_xml, 'xml')
    for i in item_info:
        if soup.find(i):
            soup.find(i).string = item_info[i]
        else:
            # We don't need to update the item's ID, but we do want to update
            # all the other fields.
            if i != 'id':
                new_tag = soup.new_tag(i)
                new_tag.string = item_info[i]
                soup.find('item_data').find('description').insert_before(new_tag)
    
    new_xml = str(soup)
    
    return new_xml
    

def update_item(base_url, mms_id, holdings_id, item_id, api_key, item_xml):
    headers = {'content-type':'application/xml'}
    query = 'bibs/{}/holdings/{}/items/{}?apikey={}'
    
    url = ''.join([base_url, query.format(mms_id, holdings_id, item_id, api_key)])
    requests.put(url, headers=headers, data=item_xml.encode('utf-8'))

    
def update(mms_id, input_file, api_key, base_url):
    items_to_update = get_info_from_csv(input_file)

    for holdings in items_to_update:
        for item in items_to_update[holdings]:
            xml = get_item_xml(base_url, mms_id, holdings, item['id'], api_key)
            updated_xml = update_item_xml(xml, item)
            update_item(base_url, mms_id, holdings, item['id'], api_key, updated_xml)